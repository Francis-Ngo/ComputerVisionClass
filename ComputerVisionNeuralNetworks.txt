In this video we will discuss Neural Networks Consider the following non-linearly separable dataset. We will use one dimension for simplicity Let's look at an example of classification where we overlaid the color over the feature, in the context of neural networks its helpful to think of the classification problem as a decision function just like a function when y equals one the value is mapped to one on the vertical axis we can represent the function like a box , this box function is an example of a decision function any values of x in the following region is one, any value of x in this region is mapped to zero A neural network will approximate this function using learnable parameters We can also view the problem as trying to approximate the box function using logistic regression anything in this region y will be a one i.e dog anything in this region y will be a 0 i.e cat If this was our cat-dog dataset, in this example we cannot use a straight line to separate the data This line can be used to linearly  separate some of the data, but some of the data is on the wrong side of the line we can use the following node to represent the line and the edges to represent the input x and output  z If we apply the logistic function, in the context of neural networks this is called the activation function These values of the function are incorrect and we get an incorrect result  in this region We can represent the sigmoid function with the following node taking the input z from the linear function and producing an output, technically  “A” is a function of z and x We will call the function “A” the activation function and the output of  of  “A”  is called the activation This line can also be used to linearly separate some of the data but some of the data is on the wrong side of the line this line looks like it can be use to separate the data but lets see what happens when we apply the sigmoid function After applying the sigmoid or activation function, we get an incorrect result for some of the samples Consider the following sigmoid functions we call them "A sub script one” and “A sub script two” If we subtract the second sigmoid function from the first sigmoid function we get something similar to the decision function We can also apply the following operations with a linear function i.e just subtract the second activations from the first activation function. These values will be learnable parameters If we apply a threshold setting every value less than 0.5 to zero and greater than 0.5 to one we get the exact function we are trying to approximate We can now classify the data, we obtain the parameters via gradient descent we can use the  graph to represent the process, we apply two linear functions to x and we get two outputs to each linear function we apply a sigmoid we then a apply a second linear function to the outputs of the sigmoid we usually apply another function to the output of this linear function then apply a threshold This diagram is used to represent a two-layer neural network, we have the hidden layer Each linear function and activation is known as an artificial neuron, in this case the hidden layer has two artificial neurons the output layer has one artificial neuron, as it has two inputs the input dimension for this neuron is two It's helpful to look at the output components of the activation, the outputs of the activation function is a 2D plane that looks like this these red data points get mapped to these points  in the 2D plane these blue  data points get mapped to these points  in the 2D plane and so on It turns out that we can split the point using the following plane this is what the linear function on the second layer does In the same way we can add more dimensions to the input, notice that there are a lot more weights between the input layer and hidden layer, we will leave out the bias terms, we see a neural networks had a lot of learnable parameters, for example a logistic regression model may have hundred's  of learnable parameters; a modern neural network will have millions. Generally these type of Neural Networks are called Feedforward Neural Networks or fully connected networks, but we will usually refer to them as Neural Networks We can use neural networks to classify multiple dimensions. Here we have a non-linearly separable dataset in two dimensions Here we have a neural network with 2 dimensions. The more dimensions the more Neurons  we require We can plot the decision function in 2 dimensions, the horizontal axis is the value y-hat, we have cats that are mapped to zero we have dogs that are mapped to one. Check out the labs for more