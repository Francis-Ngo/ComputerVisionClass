In this video, you will learn about different CNN Architectures popular architectures include: LeNet-5, AlexNet, VGGNet, ResNet We will go over a few of them in this session. We will also cover Transfer Learning One of the first CNN's was proposed by Yann LeCun et al. in 1989 The most successful use case of the LeNet-5 is the MNIST Dataset of handwritten digits LeNet receives an input image, normally a grayscale image, uses a 5 by 5 filter with a stride 1 and results in a volume of 28 by 28 outputs. The next layer is a pooling layer with 14 by 14 outputs. It repeats itself with a filter and pooling layer until it gets to the fully connected layers where it flattens to create 120 neurons and another with 84 neurons while using a sigmoid activation function to produce an output. For a while CNN's dropped out of popularity for image classification, and support vector machines became the default standard If you want to compare any image classification methods, you compare the classification accuracy on a dataset. ImageNet is a benchmark dataset i.e this is the one that everyone uses to see who has the best image classification method. Here is an image of the top performer every year. Prior to 2012, a method using SIFT a feature like HOG. took the top spot  with 51% accuracy, after 2012 AlexNet smashed this record with 63.3% accuracy. This jump was so large everyone started using CNN's for image classification , Here is a block diagram of AlexNet we see the network has lots of parameters, we see convolution kernels are of different sizes. If you recall, more parameters means  you require more data. We see the first  convolution kernel layers are of  shape 11x11 with 25 channels, this is quite a large number of parameters The VGG Network  is a Very Deep Convolutional Network that was developed out of the need to reduce the number of parameters in the Convolution layers and improve on training time. It also showed in general that deeper networks performed better VGGNet has multiple variants like the VGG 19 and VGG 16, where 16 stands for the number of layers in the network. Here VGG-16 is pictured next  to AlexNet, we see it’s much deeper. The key insight gained by the VGG networks is we could replace the larger kernels in the convolution layer by stacking convolution layers with 3 by 3 kernels, keeping the same receptive field while reducing the number of parameters. This also reduced the number of computations by reducing the number of operations in the convolution layers and decreasing the feature map size As CNN's got deeper, the vanishing gradient began to become a problem again ResNet help solve the problem by introducing Residual learning: Residual layers or skip connections allows the gradient to bypass different layers, improving performance we can now build much deeper Networks. Here is a 32 layer network from the paper Deep Residual Learning for Image Recognition Transfer learning is where you use a pre-trained CNN to classify an image Instead of building your own network, you can use the CNN Architectures we discussed, Pre-trained CNN's have been  trained with vast amount of data. We will cover the simplest method, where we replace the SoftMax layer with our own SoftMax layer. The number of neurons is equal to the number of classes, we then train the SoftMax layer on the dataset we would like to classify the input dimension of each neuron in the SoftMax layer is equal to the last number of neurons  fully connected layer. The Pretrained model can be thought of as a feature generator, depending on the size of your dataset, you can use SVM instead of the SoftMax layer. Check out the labs for more.