In this video we will cover Fully Connected Neural Network Architecture, This deals with how to arrange the different number of hidden layers and neurons. neural networks are usually represented without the learnable parameters in this example the hidden layer has four neurons, the output layer has one neuron We can make multiclass predictions using neural networks, we just add more neurons to the output layer. The process can be  thought of as  just replacing the output layer with a SoftMax function, here the output  layer has three neurons for three classes. For a given  input we obtain an output for each neuron We choose the class according to the index of the neuron that has the largest value. In this case neuron two has the the largest value, so the output of our model is two We can use the following diagram we have 5 neurons in the output layer, one for each class: dog, cats and so on We can add hidden layers. Here we have two hidden layers. If we have more than one hidden layer the neural network is called a deep neural network. More neurons or more layers may lead to overfitting The output or activation of each layer is the same dimension as the number of neurons, this layer has three neurons, the output or activation has three dimensions Each neuron is like a linear classifier, therefore each neuron must have the same number of inputs as the previous layer. In this case the previous layer has three neurons, so this neuron has three inputs Let’s see how the following neural network makes a prediction in the following layers, consider the following input vector with four dimensions. Each neuron in the 1st layer has 4 inputs as there are three neurons, the activation has a dimension of three each neuron in the next layer has an input dimension of 3 as there is 2 neurons in the second layer the output activation has a dimension of two One way to select a Fully Connected Neural Network Architecture is to use the validation data consider the following network Architecture  A , B and C we select the Architecture with the best performance  on the validation  data, in this case C It turns out that deep networks work better, but are hard to train If you recall, to perform gradient descent to obtain our learning parameters, we have to calculate the gradient, but the deeper the network, the smaller the gradient gets this is called the vanishing gradient. As a result its harder to train the deeper layers of the network One of the main drawbacks with using the sigmoid activation function is the vanishing gradient One activation function is the rectified linear unit function or Relu function for short The value of the relu function is 0 when its input is less then zero. Relu is only used in the hidden layers if the input z is larger than 0 the input of the function will equal its output if the input z equals 5, the output equals 5 if the input z equals 10, the output equals 10 Networks have layers that help with training, we will skip the details, but some methods like dropout  prevent overfitting, batch normalization to help with training skip connections allow you too train deeper Networks by connecting deeper layers during training The hidden Layers of Neural networks replace the Kernels is SVM’s.  We can use the raw  Image or features like HOG Training neural networks is more of an art than a science, so we will use the lab to try out different methods, generally Neural networks are trained in a similar manner to logistic regression and Softmax The Loss or cost surface is complicated making training difficult In the lab: we will explore more advanced variants of gradient descent Other Advanced Methods to prevent overfitting. That's it, check out the lab for more